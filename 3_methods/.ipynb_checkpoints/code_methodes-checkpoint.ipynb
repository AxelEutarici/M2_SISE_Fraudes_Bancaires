{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ca012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import zipfile\n",
    "import pickle\n",
    "import mlflow.sklearn\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#lib de metriques\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, f1_score, make_scorer, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082908f6",
   "metadata": {},
   "source": [
    "# Commandes pour telecharger les données depuis kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1114abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_username = ''\n",
    "kaggle_key = ''\n",
    "path = ''\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be165f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- username is now set to: paulineattal\n",
      "- key is now set to: 842753570fc0411f2cc69d2e7a242157\n",
      "fraudes-bancaires-smotetomek10.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle config set -n username -v $kaggle_username\n",
    "!kaggle config set -n key -v $kaggle_key\n",
    "!kaggle datasets download -d axeltrc/fraudes-bancaires-smotetomek10 -p $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95a1e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with zipfile.ZipFile('fraudes-bancaires-smotetomek10.zip', 'r') as zip_ref :\n",
    "    zip_ref.extractall()\n",
    "    for file in os.listdir():\n",
    "        Xtrain = pd.read_csv('Xtrain_SMOTETomek.csv', sep=\",\")\n",
    "        ytrain = pd.read_csv('ytrain_SMOTETomek.csv', sep=\",\")\n",
    "        ytest = pd.read_csv('ytest.csv', sep=\",\")\n",
    "        Xtest = pd.read_csv('Xtest.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef85bb",
   "metadata": {},
   "source": [
    "Cellule a executer pour diminuer le jeu de données : prend 10% de la base train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b6af7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3337616\n",
      "333762\n"
     ]
    }
   ],
   "source": [
    "index_train = Xtrain.index.to_list()\n",
    "print(len(index_train))\n",
    "random_index = random.sample(index_train, round(len(index_train)*0.1))\n",
    "print(len(random_index))\n",
    "Xtrain = Xtrain.loc[random_index,]\n",
    "ytrain = ytrain.loc[random_index,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e6fa71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest.reset_index(drop=True, inplace=True)\n",
    "Xtest.reset_index(drop=True, inplace=True)\n",
    "ytrain.reset_index(drop=True, inplace=True)\n",
    "Xtrain.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61d5f7",
   "metadata": {},
   "source": [
    "## Boucle de recherche sur meilleur modele avec les meilleurs hyperparametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ac9932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/541795256308840198', creation_time=1673438592181, experiment_id='541795256308840198', last_update_time=1673438592181, lifecycle_stage='active', name='Fouilles de Données Massives', tags={}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Fouilles de Données Massives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c0db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# On stocke ci-dessous les valeurs des hyper-paramètres que l'on souhaite tester\n",
    "#np.arange(start = 5, stop = 500, step = 50)\n",
    "params_modeles = [\n",
    "{\"loss\":[\"log_loss\"],\n",
    " \"learning_rate\":[0.09, 0.11],\n",
    " \"n_estimators\":[100],\n",
    " \"min_samples_split\":[5]}\n",
    "]\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "modeles_list = [\n",
    "    GradientBoostingClassifier()\n",
    "]\n",
    "\n",
    "\n",
    "def select_model(modeles, parameters, Xtrain, ytrain, Xtest, ytest) :\n",
    "    df = pd.DataFrame(columns = ['best','score', 'ftest', 'ftrain','rappel', 'precision', 'lr_auc', 'lr_precision', 'lr_recall', 'time_train'])    \n",
    "    \n",
    "    for i, modele in enumerate(modeles):\n",
    "        modele_name = str(modele)\n",
    "        \n",
    "        #start run mlflow\n",
    "        my_run = mlflow.start_run(run_name = modele_name)\n",
    "        \n",
    "        f1 = make_scorer(f1_score , average='macro')\n",
    "        #test all models with CV\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        model = GridSearchCV(estimator=modele,\n",
    "                            param_grid=parameters[i],\n",
    "                            scoring = f1,\n",
    "                            verbose = False,\n",
    "                            cv = 3)\n",
    "        start_time = time.time()\n",
    "        model.fit(Xtrain, ytrain)\n",
    "        full_time = time.time() - start_time\n",
    "        \n",
    "        rankTrain = model.predict(Xtrain)\n",
    "        rankTest = model.predict(Xtest)\n",
    "\n",
    "        #calcul metrics\n",
    "        #calcul de la f-mesure pour mesurer la performance du modele \n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        rappel = round(ctrain[1,1]/(ctrain[1,1]+ctrain[1,0]),4)\n",
    "        precision = round(ctrain[1,1]/(ctrain[1,1]+ctrain[0,1]),4)\n",
    "        ftrain = round(2*ctrain[1,1]/(2*ctrain[1,1]+ctrain[0,1]+ctrain[1,0]),4)\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "        #calcul de l-AUC Precision-Rappel\n",
    "        lr_precision, lr_recall, _ = precision_recall_curve(ytest, rankTest)\n",
    "        lr_auc =  auc(lr_recall, lr_precision)\n",
    "        \n",
    "        #df with all indicators\n",
    "        df.loc[i]=[model.best_estimator_, model.best_score_, ftest, ftrain, rappel, precision, lr_auc, lr_precision, lr_recall, full_time]\n",
    "        \n",
    "        #save info i mlflow\n",
    "        mlflow.sklearn.log_model(modele,modele_name)\n",
    "        #artifact\n",
    "        #mlflow.log_artifact(\"guillaume.txt\")\n",
    "        #stocker les métriques\n",
    "        my_run.metrics = {}\n",
    "        #rajout des éléments de performance\n",
    "        my_run.metrics['best'] = model.best_score_\n",
    "        my_run.metrics['ftest'] = ftest\n",
    "        my_run.metrics['ftrain'] = ftrain\n",
    "        my_run.metrics['precision'] = precision\n",
    "        my_run.metrics['rappel'] = rappel\n",
    "        my_run.metrics['lr_auc']= lr_auc\n",
    "        my_run.metrics['full_time']=full_time\n",
    "        mlflow.log_metrics(my_run.metrics)\n",
    "        #paramètres de l'algo\n",
    "        mlflow.log_params(modele.get_params())\n",
    "        mlflow.end_run()\n",
    "        \n",
    "    #return df with all indicators  \n",
    "    return df\n",
    "\n",
    "df_ind = select_model(modeles_list, params_modeles, Xtrain, ytrain, Xtest, ytest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae91b97",
   "metadata": {},
   "source": [
    "# recuperer le modele depuis mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca777ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1067\n"
     ]
    }
   ],
   "source": [
    "path_save = ''\n",
    "os.chdir(path_save)\n",
    "\n",
    "run_id = \"a76b8d4468c44427a007505886a930ee\"\n",
    "artifact_path = \"artifacts/GradientBoostingClassifier()/model.pkl\"\n",
    "artifact_path_bis = 'mlflow-artifacts:/541795256308840198/a76b8d4468c44427a007505886a930ee/artifacts/GradientBoostingClassifier()'\n",
    "\n",
    "model = mlflow.sklearn.load_model(artifact_path_bis)\n",
    "\n",
    "#entrainer sur toutes les donnees \n",
    "#Xtrain = pd.read_csv('Xtrain_SMOTETomek.csv', sep=\",\")\n",
    "#ytrain = pd.read_csv('ytrain_SMOTETomek.csv', sep=\",\")\n",
    "model.fit(Xtrain, ytrain)\n",
    "#sauvegarder sur son ordinateur\n",
    "with open('model.dat', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "\n",
    "####tester la f mesure \n",
    "rankTest = model.predict(Xtest)\n",
    "#calcul metrics\n",
    "#calcul de la f-mesure pour mesurer la performance du modele \n",
    "ctest = confusion_matrix(ytest, rankTest)\n",
    "ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "print(ftest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb0578",
   "metadata": {},
   "source": [
    "# recuperer le modele depuis le DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091168bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ind' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2824\\212089212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#appliquer et sauvegarder le meilleur modele\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#get index of df wich have best f1 score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mindice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ftest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#keep model with best f score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"best\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_ind' is not defined"
     ]
    }
   ],
   "source": [
    "#appliquer et sauvegarder le meilleur modele\n",
    "#get index of df wich have best f1 score\n",
    "indice=df_ind['ftest'].idxmax()\n",
    "#keep model with best f score\n",
    "best_model = df_ind[\"best\"][indice]\n",
    "\n",
    "Xtrain = pd.read_csv('Xtrain_SMOTETomek.csv', sep=\",\")\n",
    "ytrain = pd.read_csv('ytrain_SMOTETomek.csv', sep=\",\")\n",
    "best_model.fit(Xtrain, ytrain)\n",
    "#sauvegarder sur son ordinateur\n",
    "with open('model.dat', 'wb') as f:\n",
    "    pickle.dump(mon_modele, f)\n",
    "\n",
    "\n",
    "####tester la f mesure\n",
    "rankTest = model.predict(Xtest)\n",
    "#calcul metrics\n",
    "#calcul de la f-mesure pour mesurer la performance du modele \n",
    "ctest = confusion_matrix(ytest, rankTest)\n",
    "ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "print(ftest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374fd435",
   "metadata": {},
   "source": [
    "# Appliquer un modele deja entrainé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "947391eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1067\n"
     ]
    }
   ],
   "source": [
    "path_save = ''\n",
    "os.chdir(path_save)\n",
    "with open('model.dat', 'rb') as f:\n",
    "    mon_modele = pickle.load(f)\n",
    "    \n",
    "rankTest = mon_modele.predict(Xtest)\n",
    "#calcul metrics\n",
    "#calcul de la f-mesure pour mesurer la performance du modele \n",
    "ctest = confusion_matrix(ytest, rankTest)\n",
    "ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "print(ftest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
