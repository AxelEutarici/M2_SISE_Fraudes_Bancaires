{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc5062-276e-4005-85a4-12ea1a5fc35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "#rendre silencieux les messages du GIT\n",
    "os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060c103-67f0-44e1-8474-38089bb8cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#définir une expérimentation\n",
    "mlflow.set_experiment(\"Fouilles de Données Massives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c946e9-40a7-497a-872b-61ff5bcdfea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciation d'un run\n",
    "my_run = mlflow.start_run(run_name = 'Arbre standard')\n",
    "dir(my_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01941a0b-4c06-4446-9ebb-0bb8382841aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rajouter le champs \"metrics\" vide pour l'instant\n",
    "my_run.metrics = {}\n",
    "\n",
    "#rajout des éléments de performance\n",
    "my_run.metrics['Accuracy'] = acc\n",
    "my_run.metrics['Recall'] = rap\n",
    "my_run.metrics['Precision'] = prec\n",
    "my_run.metrics['F_Score']= fscore\n",
    "\n",
    "#\n",
    "dir(my_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ebfe85-6bf4-4319-8e78-adc47f20444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rajouter le modèle dans le log\n",
    "mlflow.sklearn.log_model(arbre_1,\"Modele Arbre\")\n",
    "\n",
    "#artifact\n",
    "mlflow.log_artifact(\"guillaume.txt\")\n",
    "\n",
    "#stocker les métriques\n",
    "mlflow.log_metrics(my_run.metrics)\n",
    "\n",
    "#paramètres de l'algo\n",
    "mlflow.log_params(arbre_1.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd3ac2-e953-426d-b29a-415e657cb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finaliser l'expérimentation\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859adf6-7607-4fc8-8fbd-a4aedb7591b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lancer les runs si on veut faire une seule boucle\n",
    "for modele in liste:\n",
    "    #le run\n",
    "    therun = mlflow.start_run(run_name = modele[0])    \n",
    "    #entraînement du modèle\n",
    "    themodele = modele[1].fit(XTrain,yTrain)\n",
    "    #prédiction\n",
    "    predmodele = themodele.predict(XTest)\n",
    "    #récupérer les résultats\n",
    "    therun.metrics = {}\n",
    "    therun.metrics['Accuracy'] = metrics.accuracy_score(yTest,predmodele)\n",
    "    therun.metrics['Rappel'] = metrics.recall_score(yTest,predmodele,pos_label='good')\n",
    "    therun.metrics['Precision'] = metrics.precision_score(yTest,predmodele,pos_label='good')\n",
    "    #rajouter le modèle dans le log\n",
    "    mlflow.sklearn.log_model(themodele,modele[0])\n",
    "    #info sur les donnees\n",
    "    mlflow.log_artifact(\"working_conditions.xlsx\")\n",
    "    #stocker les métriques\n",
    "    mlflow.log_metrics(therun.metrics)\n",
    "    #paramètres de l'algo\n",
    "    mlflow.log_params(themodele.get_params())\n",
    "    #finaliser\n",
    "    mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
