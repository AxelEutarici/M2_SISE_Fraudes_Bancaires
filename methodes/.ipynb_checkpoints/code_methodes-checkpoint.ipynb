{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "123d2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\cornuch\\Desktop\\SISE\\Fouille_donnees_massives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix, mean_squared_error, recall_score, f1_score, make_scorer,precision_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from functions import loadCsv, oneHotEncodeColumns, data_recovery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c0db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 1\n",
    "\n",
    "# On stocke ci-dessous les valeurs des hyper-paramètres que l'on souhaitent tester\n",
    "#np.arange(start = 5, stop = 250, step = 50)\n",
    "params_modeles = [\n",
    "{'estimator__max_features' : [5,10,20],\n",
    "'estimator__min_samples_split' : [50],\n",
    "'estimator__max_depth':[1000,30],\n",
    "'estimator__min_samples_leaf':[10]},\n",
    "{\"estimator__solver\":[\"newton-cg\"],\n",
    " \"estimator__penalty\":[\"none\"],\n",
    " \"estimator__max_iter\":[10000]},\n",
    "{\"estimator__kernel\":[\"linear\"]}\n",
    "]\n",
    "\n",
    "modeles_list = [\n",
    "    DecisionTreeClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "# On va stocker les résultats obtenus pour chaque jeux de données et pour chaque algorithme\n",
    "results = {}\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "#train test split\n",
    "Xtrain = pd.read_csv(\"smote_Xtrain.csv\", sep=\",\")\n",
    "ytrain = pd.read_csv(\"smote_ytrain.csv\", sep=\",\")\n",
    "Xtest = pd.read_csv(\"Xtest.csv\", sep=\",\")\n",
    "ytest = pd.read_csv(\"ytest.csv\", sep=\",\")\n",
    "Xtrain.info()\n",
    "train = pd.concat([Xtrain, ytrain], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def select_model(modeles, parameters, Xtrain, ytrain, Xtest, ytest) :\n",
    "    df = pd.DataFrame(columns = ['best','score', 'ft_imp', 'ftest', 'ftrain'])\n",
    "    \n",
    "    #normaliser\n",
    "    normalizer = Normalizer() \n",
    "    normalizer.fit(Xtrain)    \n",
    "    Xtrain = normalizer.transform(Xtrain)\n",
    "    Xtest = normalizer.transform(Xtest)\n",
    "    \n",
    "    \n",
    "    for i, modele in enumerate(modeles):\n",
    "        f1 = make_scorer(f1_score , average='macro')\n",
    "        #select only usefull features\n",
    "        sel = RFECV(estimator=modele, step=1, cv=5,scoring = f1)\n",
    "        #test all models with CV\n",
    "        model = GridSearchCV(estimator=sel,\n",
    "                            param_grid=parameters[i],\n",
    "                            scoring = f1,\n",
    "                            verbose = False,\n",
    "                            cv = 5)\n",
    "        model.fit(XTrain, yTrain)\n",
    "        \n",
    "        rankTrain = model.predict(Xtrain)\n",
    "        rankTest = model.predict(Xtest)\n",
    "\n",
    "        #calcul de la f-mesure pour mesurer la performance du modele \n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        ftrain = round(2*ctrain[1,1]/(2*ctrain[1,1]+ctrain[0,1]+ctrain[1,0]),4)\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "\n",
    "        #calcul indicators \n",
    "        pred = model.predict(XTest)\n",
    "        features=list(X.columns[model.best_estimator_.support_])\n",
    "        #df with all indicators\n",
    "        df.loc[i]=[model.best_estimator_, model.best_score_, features, ftest, ftrain]\n",
    "    #return df with all indicators  \n",
    "    return df\n",
    "\n",
    "df_ind = select_model(modeles_list, params_modeles, X_smote, y_smote)\n",
    "\n",
    "\n",
    "#appliquer et sauvegarder le meilleur modele\n",
    "#get index of df wich have best f1 score\n",
    "indice=df_ind['ftest'].idxmax()\n",
    "#keep model with best f score\n",
    "best_model = df_ind[\"best\"][indice]\n",
    "best_model.fit(X, y)\n",
    "pred = best_model.predict(X_sub)\n",
    "pickle.dump(best_model, open(\"./model.pickle.dat\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72025428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9c343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
