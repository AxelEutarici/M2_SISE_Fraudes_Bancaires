{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36ca012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import zipfile\n",
    "import pickle\n",
    "import mlflow.sklearn\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#lib de metriques\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, f1_score, make_scorer, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962be61",
   "metadata": {},
   "source": [
    "# Commandes pour telecharger les données depuis kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "06dca257",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_username = 'paulineattal'\n",
    "kaggle_key = '842753570fc0411f2cc69d2e7a242157'\n",
    "path = 'C:/Users/pauli/Documents/M2/fouille_de_donnees/projet/fichiers/kaggle'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7be165f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- username is now set to: paulineattal\n",
      "- key is now set to: 842753570fc0411f2cc69d2e7a242157\n",
      "fraudes-bancaires-smotetomek10.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle config set -n username -v $kaggle_username\n",
    "!kaggle config set -n key -v $kaggle_key\n",
    "!kaggle datasets download -d axeltrc/fraudes-bancaires-smotetomek10 -p $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0758c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with zipfile.ZipFile('fraudes-bancaires-smotetomek10.zip', 'r') as zip_ref :\n",
    "    zip_ref.extractall()\n",
    "    for file in os.listdir():\n",
    "        Xtrain = pd.read_csv('Xtrain_SMOTETomek.csv', sep=\",\")\n",
    "        ytrain = pd.read_csv('ytrain_SMOTETomek.csv', sep=\",\")\n",
    "        ytest = pd.read_csv('ytest.csv', sep=\",\")\n",
    "        Xtest = pd.read_csv('Xtest.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d42261",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = Xtrain.index.to_list()\n",
    "print(len(index_train))\n",
    "random_index = random.sample(index_train, round(len(index_train)*0.1))\n",
    "print(len(random_index))\n",
    "Xtrain = Xtrain.loc[random_index,]\n",
    "ytrain = ytrain.loc[random_index,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6e6fa71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest.reset_index(drop=True, inplace=True)\n",
    "Xtest.reset_index(drop=True, inplace=True)\n",
    "ytrain = ytrain.reset_index(drop=True)\n",
    "Xtrain = Xtrain.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61d5f7",
   "metadata": {},
   "source": [
    "## Boucle de recherche sur meilleur modele avec les meilleurs hyperparametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11c0db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Fouilles de Données Massives\")\n",
    "\n",
    "# On stocke ci-dessous les valeurs des hyper-paramètres que l'on souhaite tester\n",
    "#np.arange(start = 5, stop = 250, step = 50)\n",
    "params_modeles = [\n",
    "{\"loss\":[\"log_loss\"],\n",
    " \"learning_rate\":[0.09],\n",
    " \"n_estimators\":[200],\n",
    " \"min_samples_split\":[2]}\n",
    "]\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "modeles_list = [\n",
    "    GradientBoostingClassifier()\n",
    "]\n",
    "\n",
    "\n",
    "def select_model(modeles, parameters, Xtrain, ytrain, Xtest, ytest) :\n",
    "    df = pd.DataFrame(columns = ['best','score', 'ftest', 'ftrain','rappel', 'precision', 'lr_auc', 'lr_precision', 'lr_recall', 'time_train'])    \n",
    "    \n",
    "    for i, modele in enumerate(modeles):\n",
    "        modele_name = str(modele)\n",
    "        \n",
    "        #start run mlflow\n",
    "        my_run = mlflow.start_run(run_name = modele_name)\n",
    "        \n",
    "        f1 = make_scorer(f1_score , average='macro')\n",
    "        #test all models with CV\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        model = GridSearchCV(estimator=modele,\n",
    "                            param_grid=parameters[i],\n",
    "                            scoring = f1,\n",
    "                            verbose = False,\n",
    "                            cv = 3)\n",
    "        start_time = time.time()\n",
    "        model.fit(Xtrain, ytrain)\n",
    "        full_time = time.time() - start_time\n",
    "        \n",
    "        rankTrain = model.predict(Xtrain)\n",
    "        rankTest = model.predict(Xtest)\n",
    "\n",
    "        #calcul metrics\n",
    "        #calcul de la f-mesure pour mesurer la performance du modele \n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        rappel = round(ctrain[1,1]/(ctrain[1,1]+ctrain[1,0]),4)\n",
    "        precision = round(ctrain[1,1]/(ctrain[1,1]+ctrain[0,1]),4)\n",
    "        ftrain = round(2*ctrain[1,1]/(2*ctrain[1,1]+ctrain[0,1]+ctrain[1,0]),4)\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "        #calcul de l-AUC Precision-Rappel\n",
    "        lr_precision, lr_recall, _ = precision_recall_curve(ytest, rankTest)\n",
    "        lr_auc =  auc(lr_recall, lr_precision)\n",
    "        \n",
    "        #df with all indicators\n",
    "        df.loc[i]=[model.best_estimator_, model.best_score_, ftest, ftrain, rappel, precision, lr_auc, lr_precision, lr_recall, full_time]\n",
    "        \n",
    "        #save info i mlflow\n",
    "        mlflow.sklearn.log_model(modele,modele_name)\n",
    "        #artifact\n",
    "        #mlflow.log_artifact(\"guillaume.txt\")\n",
    "        #stocker les métriques\n",
    "        my_run.metrics = {}\n",
    "        #rajout des éléments de performance\n",
    "        my_run.metrics['best'] = model.best_score_\n",
    "        my_run.metrics['ftest'] = ftest\n",
    "        my_run.metrics['ftrain'] = ftrain\n",
    "        my_run.metrics['precision'] = precision\n",
    "        my_run.metrics['rappel'] = rappel\n",
    "        my_run.metrics['lr_auc']= lr_auc\n",
    "        my_run.metrics['full_time']=full_time\n",
    "        mlflow.log_metrics(my_run.metrics)\n",
    "        #paramètres de l'algo\n",
    "        mlflow.log_params(modele.get_params())\n",
    "        mlflow.end_run()\n",
    "        \n",
    "    #return df with all indicators  \n",
    "    return df\n",
    "\n",
    "df_ind = select_model(modeles_list, params_modeles, Xtrain, ytrain, Xtest, ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "26155003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>score</th>\n",
       "      <th>ftest</th>\n",
       "      <th>ftrain</th>\n",
       "      <th>rappel</th>\n",
       "      <th>precision</th>\n",
       "      <th>lr_auc</th>\n",
       "      <th>lr_precision</th>\n",
       "      <th>lr_recall</th>\n",
       "      <th>time_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>0.859674</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.7321</td>\n",
       "      <td>0.5854</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.117885</td>\n",
       "      <td>[0.008794358124244893, 0.12654379631388943, 1.0]</td>\n",
       "      <td>[1.0, 0.10132359653126426, 0.0]</td>\n",
       "      <td>1430.800826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                best     score   ftest  \\\n",
       "0  ([DecisionTreeRegressor(criterion='friedman_ms...  0.859674  0.1125   \n",
       "\n",
       "   ftrain  rappel  precision    lr_auc  \\\n",
       "0  0.7321  0.5854     0.9769  0.117885   \n",
       "\n",
       "                                       lr_precision  \\\n",
       "0  [0.008794358124244893, 0.12654379631388943, 1.0]   \n",
       "\n",
       "                         lr_recall   time_train  \n",
       "0  [1.0, 0.10132359653126426, 0.0]  1430.800826  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a4903",
   "metadata": {},
   "source": [
    "# recuperer le modele depuis mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ac8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = 'C:/Users/pauli/Documents/M2/fouille_de_donnees/projet/SISE_Fraudes_Bancaires/methodes'\n",
    "os.chdir(path_save)\n",
    "\n",
    "run_id = \"a76b8d4468c44427a007505886a930ee\"\n",
    "artifact_path = \"artifacts/GradientBoostingClassifier()/model.pkl\"\n",
    "artifact_path_bis = 'mlflow-artifacts:/541795256308840198/a76b8d4468c44427a007505886a930ee/artifacts/GradientBoostingClassifier()'\n",
    "\n",
    "model = mlflow.sklearn.load_model(artifact_path_bis)\n",
    "model.fit(Xtrain, ytrain)\n",
    "pickle.dump(model, open(\"./model.pickle.dat\", \"wb\"))\n",
    "\n",
    "\n",
    "####voir si c'est ok ... \n",
    "rankTrain = model.predict(Xtrain)\n",
    "rankTest = model.predict(Xtest)\n",
    "#calcul metrics\n",
    "#calcul de la f-mesure pour mesurer la performance du modele \n",
    "ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "rappel = round(ctrain[1,1]/(ctrain[1,1]+ctrain[1,0]),4)\n",
    "precision = round(ctrain[1,1]/(ctrain[1,1]+ctrain[0,1]),4)\n",
    "ftrain = round(2*ctrain[1,1]/(2*ctrain[1,1]+ctrain[0,1]+ctrain[1,0]),4)\n",
    "ctest = confusion_matrix(ytest, rankTest)\n",
    "ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "\n",
    "print(ftest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f230a9e",
   "metadata": {},
   "source": [
    "# recuperer le modele depuis le DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091168bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ind' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2824\\212089212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#appliquer et sauvegarder le meilleur modele\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#get index of df wich have best f1 score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mindice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ftest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#keep model with best f score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"best\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_ind' is not defined"
     ]
    }
   ],
   "source": [
    "#appliquer et sauvegarder le meilleur modele\n",
    "#get index of df wich have best f1 score\n",
    "indice=df_ind['ftest'].idxmax()\n",
    "#keep model with best f score\n",
    "best_model = df_ind[\"best\"][indice]\n",
    "best_model.fit(Xtrain, ytrain)\n",
    "pickle.dump(best_model, open(\"./model.pickle.dat\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459ea6e",
   "metadata": {},
   "source": [
    "# afficher les modeles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11192b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lancer les runs si on veut faire une seule boucle\n",
    "for modele in liste:\n",
    "    #le run\n",
    "    therun = mlflow.start_run(run_name = modele[0])    \n",
    "    #entraînement du modèle\n",
    "    themodele = modele[1].fit(XTrain,yTrain)\n",
    "    #prédiction\n",
    "    predmodele = themodele.predict(XTest)\n",
    "    #récupérer les résultats\n",
    "    therun.metrics = {}\n",
    "    therun.metrics['Accuracy'] = metrics.accuracy_score(yTest,predmodele)\n",
    "    therun.metrics['Rappel'] = metrics.recall_score(yTest,predmodele,pos_label='good')\n",
    "    therun.metrics['Precision'] = metrics.precision_score(yTest,predmodele,pos_label='good')\n",
    "    #rajouter le modèle dans le log\n",
    "    mlflow.sklearn.log_model(themodele,modele[0])\n",
    "    #info sur les donnees\n",
    "    mlflow.log_artifact(\"working_conditions.xlsx\")\n",
    "    #stocker les métriques\n",
    "    mlflow.log_metrics(therun.metrics)\n",
    "    #paramètres de l'algo\n",
    "    mlflow.log_params(themodele.get_params())\n",
    "    #finaliser\n",
    "    mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
