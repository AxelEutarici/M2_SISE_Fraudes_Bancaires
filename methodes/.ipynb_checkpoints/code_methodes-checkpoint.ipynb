{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123d2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\pauli\\Documents\\M2\\fouille de données\\projet\\fichiers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ca012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix, mean_squared_error, recall_score, f1_score, make_scorer,precision_score\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef436c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_clean.csv', sep=\",\")\n",
    "index_train = pd.read_csv('index_train.csv',sep=\",\")\n",
    "index_train = index_train[\"0\"].values.tolist()\n",
    "index_test = pd.read_csv('index_test.csv',sep=\",\")\n",
    "index_test = index_test[\"0\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e901ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prendre moins de lignes ??? ou l'under sampling suffira ??? \n",
    "index_train60 = random.sample(index_train, round(len(index_train)*0.6))\n",
    "index_test60 = random.sample(index_test, round(len(index_test)*0.6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d5e8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df.loc[:,\"FlagImpaye\"]\n",
    "# Drop the 'FlagImpaye' column\n",
    "X = df.drop('FlagImpaye', axis=1)\n",
    "\n",
    "#ytrain\n",
    "ytrain = y.loc[index_train,]\n",
    "ytrain60 = y.loc[index_train60,]\n",
    "#ytest\n",
    "ytest = y.loc[index_test,]\n",
    "ytest60 = y.loc[index_test60,]\n",
    "#Xtrain\n",
    "Xtrain = X.loc[index_train,]\n",
    "Xtrain60 = X.loc[index_train60,]\n",
    "#Xtest\n",
    "Xtest = X.loc[index_test,]\n",
    "Xtest60 = X.loc[index_test60,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f75116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling with tomek-link\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "print('Original dataset shape %s' % Counter(ytrain))\n",
    "tl = TomekLinks()\n",
    "Xtrain_tl, ytrain_tl = tl.fit_resample(Xtrain, ytest)\n",
    "print('Resampled dataset shape %s' % Counter(ytrain_tl))\n",
    "\n",
    "\n",
    "sm = SMOTE(sampling_strategy=0.5, k_neighbors=5, random_state=1)\n",
    "smote_Xtrain, smote_ytrain = sm.fit_resample(X, ytrain)\n",
    "smote = pd.concat([smote_Xtrain, smote_ytrain], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c0db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 1\n",
    "\n",
    "# On stocke ci-dessous les valeurs des hyper-paramètres que l'on souhaite tester\n",
    "#np.arange(start = 5, stop = 250, step = 50)\n",
    "params_modeles = [\n",
    "{'estimator__max_features' : [5,10,20],\n",
    "'estimator__min_samples_split' : [50],\n",
    "'estimator__max_depth':[1000,30],\n",
    "'estimator__min_samples_leaf':[10]},\n",
    "{\"estimator__solver\":[\"newton-cg\"],\n",
    " \"estimator__penalty\":[\"none\"],\n",
    " \"estimator__max_iter\":[10000]},\n",
    "{\"estimator__kernel\":[\"linear\"]}\n",
    "]\n",
    "\n",
    "modeles_list = [\n",
    "    DecisionTreeClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "# On va stocker les résultats obtenus pour chaque jeux de données et pour chaque algorithme\n",
    "results = {}\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "#ajouter le temps d'entraienement de chaque algo\n",
    "#en plus de sa fmesure, sa courbe auc-pr\n",
    "\n",
    "\n",
    "def select_model(modeles, parameters, Xtrain, ytrain, Xtest, ytest) :\n",
    "    df = pd.DataFrame(columns = ['best','score', 'ft_imp', 'ftest', 'ftrain', 'lr_auc', 'time_train'])\n",
    "    \n",
    "    #normaliser\n",
    "    normalizer = Normalizer() \n",
    "    normalizer.fit(Xtrain)    \n",
    "    Xtrain = normalizer.transform(Xtrain)\n",
    "    Xtest = normalizer.transform(Xtest)\n",
    "    \n",
    "    \n",
    "    for i, modele in enumerate(modeles):\n",
    "        f1 = make_scorer(f1_score , average='macro')\n",
    "        #select only usefull features\n",
    "        sel = RFECV(estimator=modele, step=1, cv=5,scoring = f1)\n",
    "        #test all models with CV\n",
    "        model = GridSearchCV(estimator=sel,\n",
    "                            param_grid=parameters[i],\n",
    "                            scoring = f1,\n",
    "                            verbose = False,\n",
    "                            cv = 5)\n",
    "        start_time = time.time()\n",
    "        model.fit(XTrain, yTrain)\n",
    "        full_time = time.time() - start_time\n",
    "        \n",
    "        rankTrain = model.predict(Xtrain)\n",
    "        rankTest = model.predict(Xtest)\n",
    "\n",
    "        #calcul metrics\n",
    "        #calcul de la f-mesure pour mesurer la performance du modele \n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        ftrain = round(2*ctrain[1,1]/(2*ctrain[1,1]+ctrain[0,1]+ctrain[1,0]),4)\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "        #calcul de l-AUC Precision-Rappel\n",
    "        lr_precision, lr_recall, _ = precision_recall_curve(ytest, rankTrain)\n",
    "        lr_auc =  auc(lr_recall, lr_precision)\n",
    "        \n",
    "        #save indicators\n",
    "        features=list(X.columns[model.best_estimator_.support_])\n",
    "        #df with all indicators\n",
    "        df.loc[i]=[model.best_estimator_, model.best_score_, features, ftest, ftrain, lr_auc, full_time]\n",
    "    #return df with all indicators  \n",
    "    return df\n",
    "\n",
    "df_ind = select_model(modeles_list, params_modeles, X_smote, y_smote)\n",
    "\n",
    "\n",
    "#appliquer et sauvegarder le meilleur modele\n",
    "#get index of df wich have best f1 score\n",
    "indice=df_ind['ftest'].idxmax()\n",
    "#keep model with best f score\n",
    "best_model = df_ind[\"best\"][indice]\n",
    "best_model.fit(X, y)\n",
    "pred = best_model.predict(X_sub)\n",
    "pickle.dump(best_model, open(\"./model.pickle.dat\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72025428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Dec 25 09:25:17 2022\n",
    "\n",
    "@author: pauli\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "Xtrain = pd.read_csv(\"smote_Xtrain.csv\", sep=\",\")\n",
    "ytrain = pd.read_csv(\"smote_ytrain.csv\", sep=\",\")\n",
    "Xtrain.info()\n",
    "train = pd.concat([Xtrain, ytrain], axis=1)\n",
    "\n",
    "\n",
    "#XGBoost/ gradient tree boosting \n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "param = {\"loss\":\"log_loss\",\"learning_rate\":0.1,\"n_estimators\":100,\"min_samples_split\":2}\n",
    "gbc = GradientBoostingClassifier(loss=\"log_loss\", learning_rate=0.1, n_estimators=100, min_samples_split=2)\n",
    "gbc.fit(Xtrain, ytrain)\n",
    "\n",
    "#Nearest-Neighbor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=2, algorithm='ball_tree')\n",
    "knc.fit(Xtrain, ytrain)\n",
    "\n",
    "#Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "param = {\"criterion\":\"gini\",\"max_depth\":None,\"min_samples_split\":2,\"min_samples_leaf\":1,\"max_features\":\"sqrt\"}\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc = dtc.fit(Xtrain, ytrain)\n",
    "\n",
    "#Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param = {\"n_estimators\":100,\"criterion\":\"gini\",\"max_depth\":None,\"min_samples_split\":2,\"min_samples_leaf\":1,\"max_features\":\"sqrt\",\"oob_score\":False,\"warm_start\":False,\"max_samples\":None}\n",
    "rfc = RandomForestClassifier()\n",
    "rfc = rfc.fit(Xtrain, ytrain)\n",
    "\n",
    "#SVM\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "param = {\"kernel\":\"rbf\",\"degree\":3,}\n",
    "#ici on peut changer le noyaux\n",
    "svc = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\",degree=3))\n",
    "svc.fit(Xtrain, ytrain)\n",
    "\n",
    "#K-means\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\")\n",
    "kmeans.fit(Xtrain)\n",
    "#bof ca, on ne prend meme pas en compte les y...\n",
    "\n",
    "#Sampling\n",
    "###A FORCEMENT FAIRE !!!!!\n",
    "\n",
    "#LOF\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "lof = LocalOutlierFactor(n_neighbors=2)\n",
    "lof.fit(train) #train contient X et y \n",
    "\n",
    "#Auto-encodeurs\n",
    "#keskecé ???\n",
    "\n",
    "#Reseaux de neurones\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpc = MLPClassifier(random_state=1, max_iter=100)\n",
    "mlpc.fit(Xtrain, ytrain)\n",
    "\n",
    "#ADL\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "adl = LinearDiscriminantAnalysis()\n",
    "adl.fit(Xtrain, ytrain)\n",
    "\n",
    "#ADQ\n",
    "from sklearn.qda import QDA\n",
    "qda = QDA()\n",
    "qda.fit(Xtrain, ytrain)\n",
    "\n",
    "#Cost-sensitive learning\n",
    "#On pondère les erreurs \n",
    "#Modifier le poids de chaque classe sur le substitue de taux d’erreur \n",
    "#Attribuer un poids a chaque entrée de la matrice de confusion (cout a l’échelle de chaque classe) \n",
    "\n",
    "#Methodes ensemblistes\n",
    "#bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "param = {\"max_features\":0.5,\"max_samples\" : 0.5}\n",
    "bagging = BaggingClassifier(KNeighborsClassifier())\n",
    "\n",
    "#boosting \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "adab = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(adab, Xtrain, ytrain, cv=5)\n",
    "\n",
    "#regression logistique\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "param = {\"solver\":\"saga\",\"penalty\":\"none\",\"max_iter\":100}\n",
    "logit = LogisticRegression(solver=\"saga\", penalty=\"none\", max_iter=100, random_state=1)\n",
    "logit.fit(Xtrain, ytrain)\n",
    "\n",
    "\n",
    "#Metric Learning\n",
    "from sklearn.metrics import mean_squared_error, recall_score, f1_score, make_scorer,precision_score\n",
    "#mesure de perg : \n",
    "    #Accuracy\n",
    "    #précision\n",
    "    #rappel\n",
    "    #F-mesure = (2TP)/(2TP + FN + FP)\n",
    "    #courbe ROC (AUC ROC).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9c343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
