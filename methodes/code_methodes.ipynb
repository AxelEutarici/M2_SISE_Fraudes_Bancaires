{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "123d2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\cornuch\\Desktop\\SISE\\Fouille_donnees_massives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c0db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################\n",
    "##### Fichiers contenant diverses fonctions ####\n",
    "################################################\n",
    "\n",
    "##################################\n",
    "#### Préparation des données #####\n",
    "##################################\n",
    "\n",
    "# Les fonctions suivantes permettent le chargement des données et de les mettre directement\n",
    "# au bon format pour la suite du travail\n",
    "\n",
    "\n",
    "import csv\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "# Chargement des fichiers csv\n",
    "\n",
    "def loadCsv(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            data.append(np.array(row))\n",
    "    data = np.array(data)\n",
    "    (n, d) = data.shape\n",
    "    return data, n, d\n",
    "\n",
    "# Binarisation des variables catégorielles\n",
    "\n",
    "\n",
    "def oneHotEncodeColumns(data, columnsCategories):\n",
    "    dataCategories = data[:, columnsCategories]\n",
    "    dataEncoded = OneHotEncoder(sparse=False).fit_transform(dataCategories)\n",
    "    columnsNumerical = []\n",
    "    for i in range(data.shape[1]):\n",
    "        if i not in columnsCategories:\n",
    "            columnsNumerical.append(i)\n",
    "    dataNumerical = data[:, columnsNumerical]\n",
    "    return np.hstack((dataNumerical, dataEncoded)).astype(float)\n",
    "\n",
    "# Chargement et mise en forme des données\n",
    "\n",
    "\n",
    "def data_recovery(dataset):\n",
    "    if dataset in ['abalone8', 'abalone17', 'abalone20']:\n",
    "        data = pd.read_csv(\"datasets/abalone.data\", header=None)\n",
    "        data = pd.get_dummies(data, dtype=float)\n",
    "        if dataset in ['abalone8']:\n",
    "            y = np.array([1 if elt == 8 else 0 for elt in data[8]])\n",
    "        elif dataset in ['abalone17']:\n",
    "            y = np.array([1 if elt == 17 else 0 for elt in data[8]])\n",
    "        elif dataset in ['abalone20']:\n",
    "            y = np.array([1 if elt == 20 else 0 for elt in data[8]])\n",
    "        X = np.array(data.drop([8], axis=1))\n",
    "    elif dataset in ['autompg']:\n",
    "        data = pd.read_csv(\"datasets/auto-mpg.data\", header=None, sep=r'\\s+')\n",
    "        data = data.replace('?', np.nan)\n",
    "        data = data.dropna()\n",
    "        data = data.drop([8], axis=1)\n",
    "        data = data.astype(float)\n",
    "        y = np.array([1 if elt in [2, 3] else 0 for elt in data[7]])\n",
    "        X = np.array(data.drop([7], axis=1))\n",
    "    elif dataset in ['australian']:\n",
    "        data, n, d = loadCsv('datasets/australian.data')\n",
    "        X = data[:, np.arange(d-1)].astype(float)\n",
    "        y = data[:, d-1].astype(int)\n",
    "        y[y != 1] = 0\n",
    "    elif dataset in ['balance']:\n",
    "        data = pd.read_csv(\"datasets/balance-scale.data\", header=None)\n",
    "        y = np.array([1 if elt in ['L'] else 0 for elt in data[0]])\n",
    "        X = np.array(data.drop([0], axis=1))\n",
    "    elif dataset in ['bankmarketing']:\n",
    "        data, n, d = loadCsv('datasets/bankmarketing.csv')\n",
    "        X = data[:, np.arange(0, d-1)]\n",
    "        X = oneHotEncodeColumns(X, [1, 2, 3, 4, 6, 7, 8, 10, 15])\n",
    "        y = data[:, d-1]\n",
    "        y[y == \"no\"] = \"0\"\n",
    "        y[y == \"yes\"] = \"1\"\n",
    "        y = y.astype(int)\n",
    "    elif dataset in ['bupa']:\n",
    "        data, n, d = loadCsv('datasets/bupa.dat')\n",
    "        X = data[:, np.arange(d-1)].astype(float)\n",
    "        y = data[:, d-1].astype(int)\n",
    "        y[y != 1] = 0\n",
    "    elif dataset in ['german']:\n",
    "        data = pd.read_csv(\"datasets/german.data-numeric\", header=None,\n",
    "                           sep=r'\\s+')\n",
    "        y = np.array([1 if elt == 2 else 0 for elt in data[24]])\n",
    "        X = np.array(data.drop([24], axis=1))\n",
    "    elif dataset in ['glass']:\n",
    "        data = pd.read_csv(\"datasets/glass.data\", header=None, index_col=0)\n",
    "        y = np.array([1 if elt == 1 else 0 for elt in data[10]])\n",
    "        X = np.array(data.drop([10], axis=1))\n",
    "    elif dataset in ['hayes']:\n",
    "        data = pd.read_csv(\"datasets/hayes-roth.data\", header=None)\n",
    "        y = np.array([1 if elt in [3] else 0 for elt in data[5]])\n",
    "        X = np.array(data.drop([0, 5], axis=1))\n",
    "    elif dataset in ['heart']:\n",
    "        data, n, d = loadCsv('datasets/heart.data')\n",
    "        X = data[:, np.arange(d-1)].astype(float)\n",
    "        y = data[:, d-1]\n",
    "        y = y.astype(int)\n",
    "        y[y != 2] = 0\n",
    "        y[y == 2] = 1\n",
    "    elif dataset in ['iono']:\n",
    "        data = pd.read_csv(\"datasets/ionosphere.data\", header=None)\n",
    "        y = np.array([1 if elt in ['b'] else 0 for elt in data[34]])\n",
    "        X = np.array(data.drop([34], axis=1))\n",
    "    elif dataset in ['libras']:\n",
    "        data = pd.read_csv(\"datasets/movement_libras.data\", header=None)\n",
    "        y = np.array([1 if elt in [1] else 0 for elt in data[90]])\n",
    "        X = np.array(data.drop([90], axis=1))\n",
    "    elif dataset == \"newthyroid\":\n",
    "        data, n, d = loadCsv('datasets/newthyroid.dat')\n",
    "        X = data[:, np.arange(d-1)].astype(float)\n",
    "        y = data[:, d-1].astype(int)\n",
    "        y[y < 2] = 0\n",
    "        y[y >= 2] = 1\n",
    "    elif dataset in ['pageblocks']:\n",
    "        data = pd.read_csv(\"datasets/page-blocks.data\", header=None,\n",
    "                           sep=r'\\s+')\n",
    "        y = np.array([1 if elt in [2, 3, 4, 5] else 0 for elt in data[10]])\n",
    "        X = np.array(data.drop([10], axis=1))\n",
    "    elif dataset in ['pima']:\n",
    "        data, n, d = loadCsv('datasets/pima-indians-diabetes.data')\n",
    "        X = data[:, np.arange(d-1)].astype(float)\n",
    "        y = data[:, d-1]\n",
    "        y[y != '1'] = '0'\n",
    "        y = y.astype(int)\n",
    "    elif dataset in ['satimage']:\n",
    "        data, n, d = loadCsv('datasets/satimage.data')\n",
    "        X = data[:, np.arange(d-1)].astype(float)\n",
    "        y = data[:, d-1]\n",
    "        y = y.astype(int)\n",
    "        y[y != 4] = 0\n",
    "        y[y == 4] = 1\n",
    "    elif dataset in ['segmentation']:\n",
    "        data, n, d = loadCsv('datasets/segmentation.data')\n",
    "        X = data[:, np.arange(1, d)].astype(float)\n",
    "        y = data[:, 0]\n",
    "        y[y == \"WINDOW\"] = '1'\n",
    "        y[y != '1'] = '0'\n",
    "        y = y.astype(int)\n",
    "    elif dataset == \"sonar\":\n",
    "        data, n, d = loadCsv('datasets/sonar.dat')\n",
    "        X = data[:, np.arange(d-1)].astype(float)\n",
    "        y = data[:, d-1]\n",
    "        y[y != 'R'] = '0'\n",
    "        y[y == 'R'] = '1'\n",
    "        y = y.astype(int)\n",
    "    elif dataset == \"spambase\":\n",
    "        data, n, d = loadCsv('datasets/spambase.dat')\n",
    "        X = data[:, np.arange(d-1)].astype(float)\n",
    "        y = data[:, d-1].astype(int)\n",
    "        y[y != 1] = 0\n",
    "    elif dataset == \"splice\":\n",
    "        data, n, d = loadCsv('datasets/splice.data')\n",
    "        X = data[:, np.arange(1, d)].astype(float)\n",
    "        y = data[:, 0].astype(int)\n",
    "        y[y == 1] = 2\n",
    "        y[y == -1] = 1\n",
    "        y[y == 2] = 0\n",
    "    elif dataset in ['vehicle']:\n",
    "        data, n, d = loadCsv('datasets/vehicle.data')\n",
    "        X = data[:, np.arange(d-1)].astype(float)\n",
    "        y = data[:, d-1]\n",
    "        y[y != \"van\"] = '0'\n",
    "        y[y == \"van\"] = '1'\n",
    "        y = y.astype(int)\n",
    "    elif dataset in ['wdbc']:\n",
    "        data, n, d = loadCsv('datasets/wdbc.dat')\n",
    "        X = data[:, np.arange(d-1)].astype(float)\n",
    "        y = data[:, d-1]\n",
    "        y[y != 'M'] = '0'\n",
    "        y[y == 'M'] = '1'\n",
    "        y = y.astype(int)\n",
    "    elif dataset in ['wine']:\n",
    "        data = pd.read_csv(\"datasets/wine.data\", header=None)\n",
    "        y = np.array([1 if elt == 1 else 0 for elt in data[0]])\n",
    "        X = np.array(data.drop([0], axis=1))\n",
    "    elif dataset in ['wine4']:\n",
    "        data = pd.read_csv(\"datasets/winequality-red.csv\", sep=';')\n",
    "        y = np.array([1 if elt in [4] else 0 for elt in data.quality])\n",
    "        X = np.array(data.drop([\"quality\"], axis=1))\n",
    "    elif dataset in ['yeast3', 'yeast6']:\n",
    "        data = pd.read_csv(\"datasets/yeast.data\", header=None, sep=r'\\s+')\n",
    "        data = data.drop([0], axis=1)\n",
    "        if dataset == 'yeast3':\n",
    "            y = np.array([1 if elt == 'ME3' else 0 for elt in data[9]])\n",
    "        elif dataset == 'yeast6':\n",
    "            y = np.array([1 if elt == 'EXC' else 0 for elt in data[9]])\n",
    "        X = np.array(data.drop([9], axis=1))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#### Fonctions annexes pour la cross-validation et exécution algorithmes #####\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Création d'une grille sur les différentes valeurs des hyper-paramètres\n",
    "\n",
    "def listP(dic):  \n",
    "    params = list(dic.keys())\n",
    "    listParam = [{params[0]: value} for value in dic[params[0]]]\n",
    "    for i in range(1, len(params)):\n",
    "        newListParam = []\n",
    "        currentParamName = params[i]\n",
    "        currentParamRange = dic[currentParamName]\n",
    "        for previousParam in listParam:\n",
    "            for value in currentParamRange:\n",
    "                newParam = previousParam.copy()\n",
    "                newParam[currentParamName] = value\n",
    "                newListParam.append(newParam)\n",
    "        listParam = newListParam.copy()\n",
    "    return listParam\n",
    "\n",
    "# Application des algorithmes \n",
    "\n",
    "def applyAlgo(algo, p, Xtrain, ytrain, Xtest, ytest, mesure):\n",
    "\n",
    "    # On commence par indiquer ce que l'on va faire avec chaque algorithme.\n",
    "    # on prendra soin de préciser les hyper-paramètres dont dépend l'algorithme\n",
    "\n",
    "    if algo == \"knn\":\n",
    "        clf = KNeighborsClassifier(n_neighbors=p[\"k\"])\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        rankTrain = clf.predict(Xtrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"svm_linear\":\n",
    "        clf = svm.SVC(C = p[\"C\"], kernel = 'linear')\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        rankTrain = clf.predict(Xtrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"svm_poly\":\n",
    "        clf = svm.SVC(C = p[\"C\"], degree = p[\"d\"] , kernel = 'poly')\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        rankTrain = clf.predict(Xtrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "        \n",
    "    elif algo == \"svm_gaus\":\n",
    "        clf = svm.SVC(C = p[\"C\"], kernel = 'rbf', degree = p[\"G\"])\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        rankTrain = clf.predict(Xtrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "   # Cette deuxième partie permet d'indiquer qu'elle est la mesure de performance que \n",
    "   # vous souhaitez considérer pour votre étude en cours\n",
    "\n",
    "\n",
    "    if mesure == \"f1\": # La f-mesure\n",
    "\n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        ftrain = round(2*ctrain[1,1]/(2*ctrain[1,1]+ctrain[0,1]+ctrain[1,0]),4)\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "\n",
    "    elif mesure == \"g1\": # La G-mesure\n",
    "        \n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        ftrain = round(np.sqrt( (ctrain[1,1]/(ctrain[1,1]+ctrain[0,1]))*(ctrain[1,1]/(ctrain[1,1]+ctrain[1,0])) ),4)\n",
    "        if np.isnan(ftrain):\n",
    "            ftrain = 0\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round(np.sqrt( (ctest[1,1]/(ctest[1,1]+ctest[0,1]))*(ctest[1,1]/(ctest[1,1]+ctest[1,0])) ),4)\n",
    "        if np.isnan(ftest):\n",
    "            ftest = 0\n",
    "\n",
    "    elif mesure == \"gm\": # La G-mean\n",
    "        \n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        ftrain = round(np.sqrt( (ctrain[1,1]/(ctrain[1,1]+ctrain[1,0]))*(ctrain[0,0]/(ctrain[0,0]+ctrain[0,1]))),4)\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round(np.sqrt( (ctest[1,1]/(ctest[1,1]+ctest[1,0]))*(ctest[0,0]/(ctest[0,0]+ctest[0,1]))),4)\n",
    "\n",
    "    elif mesure == \"ba\": # La Balanced Accuracy\n",
    "        \n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        ftrain = round( ctrain[1,1]/(ctrain[1,1]+ctrain[1,0]) + ctrain[0,0]/(ctrain[0,0]+ctrain[0,1]),4)\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round((ctest[1,1]/(ctest[1,1]+ctest[1,0])+ctest[0,0]/(ctest[0,0]+ctest[0,1]))/2,4)\n",
    "\n",
    "\n",
    "    return (ftrain*100, ftest*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72025428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.08% balance (625, 4)\n",
      "1\n",
      "34.90% pima (768, 8)\n",
      "1\n",
      "02.36% yeast6 (1484, 8)\n",
      "1\n",
      "14.29% segmentation (2310, 19)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### Quelques paramètrs généraux pour votre code ###\n",
    "\n",
    "nbFoldValid = 5 # nombre de groupes pour la K-CV\n",
    "seed = 1\n",
    "\n",
    "# On stocke ci-dessous les valeurs des hyper-paramètres que l'on souhaitent tester\n",
    "# La fonction listP va permetttre de créer une grille de valeurs sur les hyper-paramètres\n",
    "\n",
    "listParams = {#\"knn\": listP({\"k\": [1]}),\n",
    "      \"svm_linear\": listP({\"C\": [0.01,0.1,1,10]}),\n",
    "      \"svm_poly\": listP({\"C\": [0.01,0.1,1,10], \n",
    "      \"d\": [2,3,4]}),\n",
    "              \"svm_gaus\" : listP({\"C\" :[0.1, 0.5, 1, 2, 4],\n",
    "                                  \"G\" : [0.01, 0.1, 1, 10]})\n",
    "              }\n",
    "\n",
    "# On va stocker les résultats obtenus pour chaque jeux de données et pour chaque algorithme\n",
    "results = {}\n",
    "\n",
    "# On va maintenant parcourir l'ensemble de nos jeux de données sur lesquels\n",
    "# on applique successivement nos algorithmes\n",
    "\n",
    "#for dataset in ['glass', 'wine', 'balance', 'autompg', 'pima', 'yeast3',\n",
    "#                'yeast6', 'iono', 'hayes', 'vehicle', 'wine4', 'german',\n",
    "#                'libras', 'abalone17', 'abalone20', 'abalone8', 'segmentation',\n",
    "#                'pageblocks', 'satimage','bupa','heart','newthyroid','sonar',\n",
    "#                'spambase','splice','wdbc']:\n",
    "for dataset in ['balance', 'pima','yeast6','segmentation']:\n",
    "\n",
    "    X, y = data_recovery(dataset) # chargement des données\n",
    "    pctPos = 100*len(y[y == 1])/len(y) # calcul % positifs\n",
    "    dataset = \"{:05.2f}%\".format(pctPos) + \" \" + dataset \n",
    "    print(dataset, X.shape)\n",
    "    print(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, shuffle=True,\n",
    "                                                    stratify=y, test_size=0.2) # train-test split\n",
    "    skf = StratifiedKFold(n_splits=nbFoldValid, shuffle=True) # initialisation CV\n",
    "    foldsTrainValid = list(skf.split(Xtrain, ytrain)) # création des groupes\n",
    "    results[dataset] = {} \n",
    "    for algo in listParams.keys(): # on parcourt l'ensemble des algorithmes\n",
    "        if len(listParams[algo]) > 1:  # Début Cross-Validation\n",
    "            validParam = [] # Stockage du score moyen par éléments de notre grille hyper-paramètres\n",
    "            for param in listParams[algo]: # On parcourt l'ensemble des hyper-paramètres\n",
    "                valid = [] # stockage des scores pour sur chaque fold\n",
    "                for iFoldVal in range(nbFoldValid):\n",
    "                    fTrain, fValid = foldsTrainValid[iFoldVal] # définition du fold de validation\n",
    "                    # Normalisation\n",
    "                    normalizer = Normalizer() \n",
    "                    normalizer.fit(Xtrain[fTrain]) \n",
    "                    X_trainv = normalizer.transform(Xtrain[fTrain]) \n",
    "                    X_valid = normalizer.transform(Xtrain[fValid])\n",
    "                    # Fin normalisation\n",
    "                    # On applique notre algo \n",
    "                    valid.append(applyAlgo(algo, param,\n",
    "                                           X_trainv, ytrain[fTrain],\n",
    "                                           X_valid, ytrain[fValid],\"f1\")[1])\n",
    "                    # On stocke les valeurs \n",
    "                validParam.append(np.mean(valid))\n",
    "            param = listParams[algo][np.argmax(validParam)]\n",
    "        else:  # Pas de Cross-validation\n",
    "            param = listParams[algo][0]\n",
    "        # Normalisation\n",
    "        normalizer = Normalizer()\n",
    "        normalizer.fit(Xtrain)\n",
    "        Xtrain = normalizer.transform(Xtrain)\n",
    "        Xtest = normalizer.transform(Xtest)\n",
    "        # Fin normalisation\n",
    "        apTrain, apTest = applyAlgo(algo, param, Xtrain, ytrain, Xtest, ytest,mesure=\"f1\")\n",
    "        results[dataset][algo] = (apTrain, apTest)\n",
    "\n",
    "\n",
    "\n",
    "        # La suite n'est pas \"importante\" elle permettra de stocker vos résultats \n",
    "        # dans un tableau vous permettant de générer un pdf avec vos résultats\n",
    "\n",
    "    if not os.path.exists(\"results\"):\n",
    "        try:\n",
    "            os.makedirs(\"results\")\n",
    "        except:\n",
    "            pass\n",
    "    f = gzip.open(\"./results/res\" + str(seed) + \".pklz\", \"wb\")\n",
    "    pickle.dump(results, f)\n",
    "    f.close()\n",
    "\n",
    "# Affichage de l'ensemble des résultats\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9c343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
